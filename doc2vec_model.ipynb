{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sctokenizer in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.12 in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy==1.12) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\migue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "     ---------------------------------------- 0.0/78.6 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 41.0/78.6 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 78.6/78.6 kB 2.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: typing\n",
      "  Building wheel for typing (setup.py): started\n",
      "  Building wheel for typing (setup.py): finished with status 'done'\n",
      "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26327 sha256=1c82015a6c82c957b2a1d30f1c6fd5635bd6d0f3d1963064dcb618304e113b68\n",
      "  Stored in directory: c:\\users\\migue\\appdata\\local\\pip\\cache\\wheels\\7c\\d0\\9e\\1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
      "Successfully built typing\n",
      "Installing collected packages: typing\n",
      "Successfully installed typing-3.7.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sctokenizer\n",
    "!pip install nltk\n",
    "!pip install scipy==1.12\n",
    "!pip install gensim\n",
    "!pip install typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sctokenizer import JavaTokenizer, TokenType\n",
    "import math\n",
    "def tokenize_code(code):\n",
    "    \"\"\"\n",
    "    Tokenize Java code using JavaTokenizer from sctokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    code (str): The Java code to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of token types or token values.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = JavaTokenizer()\n",
    "    tokens = tokenizer.tokenize(code)\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "      if token.token_type == TokenType.COMMENT_SYMBOL:\n",
    "        continue\n",
    "      if token.token_type == TokenType.IDENTIFIER:\n",
    "        token_list.append(token.token_type)\n",
    "      else:\n",
    "        token_list.append(token.token_value)\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Document:\n",
    "  \"\"\"\n",
    "  Represents a document object for storing and processing text content.\n",
    "  \"\"\"\n",
    "  all_documents = []\n",
    "\n",
    "  def __init__(self, doc_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Initializes a Document object with the provided document name.\n",
    "\n",
    "    Args:\n",
    "        doc_name (str): The name of the document file.\n",
    "    \"\"\"\n",
    "    self.doc_name = doc_name\n",
    "    self.word_dict: dict[str, int] = {}\n",
    "    self.raw_text = \"\"\n",
    "    self.text = \"\"\n",
    "    self.tokens: list[str] = []\n",
    "    self.raw_comments: str = \"\"\n",
    "    self.token_comments: list[str] = []\n",
    "    self.one_line_text: str = \"\"\n",
    "\n",
    "    self.__read_text()\n",
    "    self.__create_word_dict()\n",
    "    self.__get_comments(self.raw_text)\n",
    "    self.__get_one_line_text()\n",
    "    Document.all_documents.append(self)\n",
    "\n",
    "  def __get_comments(self, code):\n",
    "    \"\"\"\n",
    "    Extracts comments from the provided code string.\n",
    "\n",
    "    Args:\n",
    "        code (str): The code string to extract comments from.\n",
    "    \"\"\"\n",
    "    single_line_comment_pattern = r'//.*'\n",
    "    multi_line_comment_pattern = r'/\\*[\\s\\S]*?\\*/'\n",
    "\n",
    "    single_line_comments = re.findall(single_line_comment_pattern, code)\n",
    "    multi_line_comments = re.findall(multi_line_comment_pattern, code)\n",
    "\n",
    "    stripped_single_line_comments = [comment.lstrip('//').strip() for comment in single_line_comments]\n",
    "    stripped_multi_line_comments = [re.sub(r'(^/\\*|\\*/$)', '', comment).strip() for comment in multi_line_comments]\n",
    "\n",
    "    all_comments = \"\"\n",
    "    for comment in stripped_single_line_comments:\n",
    "      all_comments += f\"{comment} \"\n",
    "\n",
    "    for comment in stripped_multi_line_comments:\n",
    "      all_comments += f\"{comment}\"\n",
    "\n",
    "    self.raw_comments = all_comments\n",
    "    self.token_comments = self.raw_comments.split(\" \")\n",
    "\n",
    "  def __read_text(self) -> None:\n",
    "    \"\"\"\n",
    "    Reads the text content from the document file.\n",
    "    \"\"\"\n",
    "    raw_text = \"\"\n",
    "    text = \"\"\n",
    "    file = open(f\"{self.doc_name}\", \"r\", encoding=\"utf-8\")\n",
    "    while True:\n",
    "      line = file.readline()\n",
    "      if not line:\n",
    "        break\n",
    "\n",
    "      raw_text += line\n",
    "      tokenized_line = tokenize_code(line)\n",
    "      for token in tokenized_line:\n",
    "        self.tokens.append(str(token))\n",
    "\n",
    "      text += \" \".join(str(tokenized_line))\n",
    "    file.close()\n",
    "\n",
    "    self.raw_text = raw_text\n",
    "    self.text = text\n",
    "\n",
    "  def __create_word_dict(self):\n",
    "    \"\"\"\n",
    "    Creates a dictionary to store the frequency of each word in the document.\n",
    "    \"\"\"\n",
    "    for token in self.tokens:\n",
    "      self.word_dict[token] = self.word_dict.get(token, 0) + 1\n",
    "  \n",
    "  def __get_one_line_text(self):\n",
    "    str_tokens = []\n",
    "    for token in self.tokens:\n",
    "      str_tokens.append(str(token))\n",
    "\n",
    "    self.one_line_text = \" \".join(str_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def generate_documents(class_name: str, directory: str = \"\") -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Generate a table of document comparison metrics for a given class name.\n",
    "\n",
    "    Parameters:\n",
    "    class_name (str): The name of the class.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing comparison metrics for each document pair.\n",
    "    \"\"\"\n",
    "\n",
    "    directorio_principal = directory\n",
    "\n",
    "    data = []\n",
    "\n",
    "    ruta_clase = os.path.join(directorio_principal, class_name)\n",
    "\n",
    "    if os.path.exists(ruta_clase):\n",
    "        for carpeta_id in os.listdir(ruta_clase):\n",
    "            ruta_carpeta = os.path.join(ruta_clase, carpeta_id)\n",
    "\n",
    "            if os.path.isdir(ruta_carpeta):\n",
    "                for archivo in os.listdir(ruta_carpeta):\n",
    "                    ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "                    Document(ruta_archivo)\n",
    "                    \n",
    "    else:\n",
    "        print(f'La ruta {ruta_clase} no existe')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name = \"data_set_splitted/\"\n",
    "\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}train\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}train\")\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}val\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}val\")\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}test\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}test\")\n",
    "generate_documents(\"no_plagiado\", \"queries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832\n"
     ]
    }
   ],
   "source": [
    "print(len(Document.all_documents))\n",
    "\n",
    "data = []\n",
    "\n",
    "for document in Document.all_documents:  \n",
    "  data.append(document.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "vector_size = 100\n",
    "epochs = 50\n",
    "\n",
    "tagged_data = [TaggedDocument(words=code, tags=[str(i)]) for i, code in enumerate(data)]\n",
    "\n",
    "model = Doc2Vec(vector_size=vector_size, epochs=epochs)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "model.train(tagged_data, \n",
    "  total_examples=model.corpus_count,\n",
    "  epochs=model.epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"doc_2_vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9038807   0.6516178   0.38068998 -0.21664296 -0.02243141 -0.06655913\n",
      " -0.02820565 -0.02017074  0.10825096 -0.58535016  0.5246618   0.11500695\n",
      " -0.21995214 -0.10347764  0.08916871 -0.71791947 -0.16580206 -0.32584137\n",
      "  0.1914409   0.22244132  0.03306208  0.32970122 -0.01666521  0.04224354\n",
      " -0.9864555  -0.28224176  0.49276832  0.4007685   0.13869038 -0.5953549\n",
      "  0.8812585  -1.027326   -0.53101987  0.3375814  -0.3416     -0.32526255\n",
      "  1.031309    0.34625775 -0.58447295  0.45306736 -0.31948668 -0.0484423\n",
      " -0.26935917  0.8983924  -0.2337651   0.5975756   0.14304486  0.3778272\n",
      " -0.03943252 -0.40287146 -0.5738162   0.07660487  0.36672214 -0.3848258\n",
      " -0.38750222  0.38827497  0.31332833  0.42055115 -0.27143547 -0.40805867\n",
      " -0.59332156 -0.24029462  0.67487216 -0.3452443  -0.1233667  -0.29811016\n",
      "  0.04851728 -0.6381121  -0.16599202 -0.93818843 -0.45292136  0.16069964\n",
      "  0.6289938   0.18703412 -0.20723057  0.7109115  -0.00558115 -0.19444495\n",
      " -0.68056464 -0.41690558 -0.05172285  0.12327626 -0.20856738 -0.23510262\n",
      "  0.5612001  -0.49864644  0.38955006  0.07887494 -0.07850727 -0.03622979\n",
      "  0.3229821   0.06491204 -0.02562405  0.08703331 -0.3720657  -0.42332533\n",
      "  0.14751436  0.2708904   0.41340902 -0.2927276 ]\n"
     ]
    }
   ],
   "source": [
    "code_vectors = []\n",
    "for code in data:\n",
    "  code_vectors.append(model.infer_vector(code))\n",
    "\n",
    "print(code_vectors[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
