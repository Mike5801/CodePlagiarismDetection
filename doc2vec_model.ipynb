{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sctokenizer in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.0.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scipy==1.12\n",
      "  Using cached scipy-1.12.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scipy==1.12) (1.26.4)\n",
      "Using cached scipy-1.12.0-cp312-cp312-win_amd64.whl (45.8 MB)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.12.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sctokenizer\n",
    "!pip install nltk\n",
    "!pip install scipy==1.12\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sctokenizer import JavaTokenizer, TokenType\n",
    "import math\n",
    "def tokenize_code(code):\n",
    "    \"\"\"\n",
    "    Tokenize Java code using JavaTokenizer from sctokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    code (str): The Java code to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of token types or token values.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = JavaTokenizer()\n",
    "    tokens = tokenizer.tokenize(code)\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "      if token.token_type == TokenType.COMMENT_SYMBOL:\n",
    "        continue\n",
    "      if token.token_type == TokenType.IDENTIFIER:\n",
    "        token_list.append(token.token_type)\n",
    "      else:\n",
    "        token_list.append(token.token_value)\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Document:\n",
    "  \"\"\"\n",
    "  Represents a document object for storing and processing text content.\n",
    "  \"\"\"\n",
    "  all_documents = []\n",
    "\n",
    "  def __init__(self, doc_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Initializes a Document object with the provided document name.\n",
    "\n",
    "    Args:\n",
    "        doc_name (str): The name of the document file.\n",
    "    \"\"\"\n",
    "    self.doc_name = doc_name\n",
    "    self.word_dict: dict[str, int] = {}\n",
    "    self.raw_text = \"\"\n",
    "    self.text = \"\"\n",
    "    self.tokens: list[str] = []\n",
    "    self.raw_comments: str = \"\"\n",
    "    self.token_comments: list[str] = []\n",
    "    self.one_line_text: str = \"\"\n",
    "\n",
    "    self.__read_text()\n",
    "    self.__create_word_dict()\n",
    "    self.__get_comments(self.raw_text)\n",
    "    self.__get_one_line_text()\n",
    "    Document.all_documents.append(self)\n",
    "\n",
    "  def __get_comments(self, code):\n",
    "    \"\"\"\n",
    "    Extracts comments from the provided code string.\n",
    "\n",
    "    Args:\n",
    "        code (str): The code string to extract comments from.\n",
    "    \"\"\"\n",
    "    single_line_comment_pattern = r'//.*'\n",
    "    multi_line_comment_pattern = r'/\\*[\\s\\S]*?\\*/'\n",
    "\n",
    "    single_line_comments = re.findall(single_line_comment_pattern, code)\n",
    "    multi_line_comments = re.findall(multi_line_comment_pattern, code)\n",
    "\n",
    "    stripped_single_line_comments = [comment.lstrip('//').strip() for comment in single_line_comments]\n",
    "    stripped_multi_line_comments = [re.sub(r'(^/\\*|\\*/$)', '', comment).strip() for comment in multi_line_comments]\n",
    "\n",
    "    all_comments = \"\"\n",
    "    for comment in stripped_single_line_comments:\n",
    "      all_comments += f\"{comment} \"\n",
    "\n",
    "    for comment in stripped_multi_line_comments:\n",
    "      all_comments += f\"{comment}\"\n",
    "\n",
    "    self.raw_comments = all_comments\n",
    "    self.token_comments = self.raw_comments.split(\" \")\n",
    "\n",
    "  def __read_text(self) -> None:\n",
    "    \"\"\"\n",
    "    Reads the text content from the document file.\n",
    "    \"\"\"\n",
    "    raw_text = \"\"\n",
    "    text = \"\"\n",
    "    file = open(f\"{self.doc_name}\", \"r\", encoding=\"utf-8\")\n",
    "    while True:\n",
    "      line = file.readline()\n",
    "      if not line:\n",
    "        break\n",
    "\n",
    "      raw_text += line\n",
    "      tokenized_line = tokenize_code(line)\n",
    "      for token in tokenized_line:\n",
    "        self.tokens.append(str(token))\n",
    "\n",
    "      text += \" \".join(str(tokenized_line))\n",
    "    file.close()\n",
    "\n",
    "    self.raw_text = raw_text\n",
    "    self.text = text\n",
    "\n",
    "  def __create_word_dict(self):\n",
    "    \"\"\"\n",
    "    Creates a dictionary to store the frequency of each word in the document.\n",
    "    \"\"\"\n",
    "    for token in self.tokens:\n",
    "      self.word_dict[token] = self.word_dict.get(token, 0) + 1\n",
    "  \n",
    "  def __get_one_line_text(self):\n",
    "    str_tokens = []\n",
    "    for token in self.tokens:\n",
    "      str_tokens.append(str(token))\n",
    "\n",
    "    self.one_line_text = \" \".join(str_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def generate_documents(class_name: str, directory: str = \"\") -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Generate a table of document comparison metrics for a given class name.\n",
    "\n",
    "    Parameters:\n",
    "    class_name (str): The name of the class.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing comparison metrics for each document pair.\n",
    "    \"\"\"\n",
    "\n",
    "    directorio_principal = directory\n",
    "\n",
    "    data = []\n",
    "\n",
    "    ruta_clase = os.path.join(directorio_principal, class_name)\n",
    "\n",
    "    if os.path.exists(ruta_clase):\n",
    "        for carpeta_id in os.listdir(ruta_clase):\n",
    "            ruta_carpeta = os.path.join(ruta_clase, carpeta_id)\n",
    "\n",
    "            if os.path.isdir(ruta_carpeta):\n",
    "                for archivo in os.listdir(ruta_carpeta):\n",
    "                    ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "                    Document(ruta_archivo)\n",
    "                    \n",
    "    else:\n",
    "        print(f'La ruta {ruta_clase} no existe')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name = \"data_set_splitted/\"\n",
    "\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}train\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}train\")\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}val\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}val\")\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}test\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}test\")\n",
    "generate_documents(\"no_plagiado\", \"queries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832\n"
     ]
    }
   ],
   "source": [
    "print(len(Document.all_documents))\n",
    "\n",
    "data = []\n",
    "\n",
    "for document in Document.all_documents:  \n",
    "  data.append(document.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "vector_size = 60\n",
    "epochs = 100\n",
    "\n",
    "tagged_data = [TaggedDocument(words=code, tags=[str(i)]) for i, code in enumerate(data)]\n",
    "\n",
    "model = Doc2Vec(vector_size=vector_size, epochs=epochs)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "model.train(tagged_data, \n",
    "  total_examples=model.corpus_count,\n",
    "  epochs=model.epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"doc_2_vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0704253e+00 -2.3402269e-01 -5.9948975e-01  3.9359775e-01\n",
      "  4.0985331e-01 -1.3024232e-01 -1.5597981e-01 -5.2251959e-01\n",
      " -8.7454826e-01 -8.2927752e-01  2.6500143e-02 -7.7603894e-01\n",
      " -3.4269625e-01  1.6587644e+00  4.2544621e-01 -1.0857534e+00\n",
      "  5.2693123e-01  2.5648770e-01  1.8171641e+00  2.5332472e-01\n",
      "  8.0578291e-01  2.0665821e-01  9.5942754e-01 -1.2399166e+00\n",
      " -8.0167770e-02  9.5934473e-04 -3.6394474e-01  1.3132125e+00\n",
      "  8.3329946e-01 -2.1232757e-01 -6.5241568e-03 -8.3940662e-02\n",
      " -9.5506740e-01 -4.5085961e-01 -7.8117388e-01 -7.4281543e-01\n",
      "  7.0676714e-01 -5.6994754e-01 -1.6101278e-02 -1.1873368e+00\n",
      " -3.7913325e-01  4.0421683e-01  4.6186656e-02 -1.5572073e-01\n",
      " -2.6295093e-01 -1.5952112e-02  1.5770830e+00 -1.5088477e+00\n",
      " -4.7695443e-01  1.0494606e+00 -7.0793241e-01  5.8350984e-02\n",
      "  3.0346436e-02 -1.6266169e-01 -1.0991377e-01 -1.2480173e+00\n",
      " -2.8784546e-01  5.5258179e-01 -1.4954592e+00 -1.5448141e-02]\n"
     ]
    }
   ],
   "source": [
    "code_vectors = []\n",
    "for code in data:\n",
    "  code_vectors.append(model.infer_vector(code))\n",
    "\n",
    "print(code_vectors[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
