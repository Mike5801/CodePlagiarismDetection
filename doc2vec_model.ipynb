{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sctokenizer in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.0.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scipy==1.12\n",
      "  Using cached scipy-1.12.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scipy==1.12) (1.26.4)\n",
      "Using cached scipy-1.12.0-cp312-cp312-win_amd64.whl (45.8 MB)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.12.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\migue\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sctokenizer\n",
    "!pip install nltk\n",
    "!pip install scipy==1.12\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sctokenizer import JavaTokenizer, TokenType\n",
    "import math\n",
    "def tokenize_code(code):\n",
    "    \"\"\"\n",
    "    Tokenize Java code using JavaTokenizer from sctokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    code (str): The Java code to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of token types or token values.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = JavaTokenizer()\n",
    "    tokens = tokenizer.tokenize(code)\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "      if token.token_type == TokenType.COMMENT_SYMBOL:\n",
    "        continue\n",
    "      if token.token_type == TokenType.IDENTIFIER:\n",
    "        token_list.append(token.token_type)\n",
    "      else:\n",
    "        token_list.append(token.token_value)\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Document:\n",
    "  \"\"\"\n",
    "  Represents a document object for storing and processing text content.\n",
    "  \"\"\"\n",
    "  all_documents = []\n",
    "\n",
    "  def __init__(self, doc_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Initializes a Document object with the provided document name.\n",
    "\n",
    "    Args:\n",
    "        doc_name (str): The name of the document file.\n",
    "    \"\"\"\n",
    "    self.doc_name = doc_name\n",
    "    self.word_dict: dict[str, int] = {}\n",
    "    self.raw_text = \"\"\n",
    "    self.text = \"\"\n",
    "    self.tokens: list[str] = []\n",
    "    self.raw_comments: str = \"\"\n",
    "    self.token_comments: list[str] = []\n",
    "    self.one_line_text: str = \"\"\n",
    "\n",
    "    self.__read_text()\n",
    "    self.__create_word_dict()\n",
    "    self.__get_comments(self.raw_text)\n",
    "    self.__get_one_line_text()\n",
    "    Document.all_documents.append(self)\n",
    "\n",
    "  def __get_comments(self, code):\n",
    "    \"\"\"\n",
    "    Extracts comments from the provided code string.\n",
    "\n",
    "    Args:\n",
    "        code (str): The code string to extract comments from.\n",
    "    \"\"\"\n",
    "    single_line_comment_pattern = r'//.*'\n",
    "    multi_line_comment_pattern = r'/\\*[\\s\\S]*?\\*/'\n",
    "\n",
    "    single_line_comments = re.findall(single_line_comment_pattern, code)\n",
    "    multi_line_comments = re.findall(multi_line_comment_pattern, code)\n",
    "\n",
    "    stripped_single_line_comments = [comment.lstrip('//').strip() for comment in single_line_comments]\n",
    "    stripped_multi_line_comments = [re.sub(r'(^/\\*|\\*/$)', '', comment).strip() for comment in multi_line_comments]\n",
    "\n",
    "    all_comments = \"\"\n",
    "    for comment in stripped_single_line_comments:\n",
    "      all_comments += f\"{comment} \"\n",
    "\n",
    "    for comment in stripped_multi_line_comments:\n",
    "      all_comments += f\"{comment}\"\n",
    "\n",
    "    self.raw_comments = all_comments\n",
    "    self.token_comments = self.raw_comments.split(\" \")\n",
    "\n",
    "  def __read_text(self) -> None:\n",
    "    \"\"\"\n",
    "    Reads the text content from the document file.\n",
    "    \"\"\"\n",
    "    raw_text = \"\"\n",
    "    text = \"\"\n",
    "    file = open(f\"{self.doc_name}\", \"r\", encoding=\"utf-8\")\n",
    "    while True:\n",
    "      line = file.readline()\n",
    "      if not line:\n",
    "        break\n",
    "\n",
    "      raw_text += line\n",
    "      tokenized_line = tokenize_code(line)\n",
    "      for token in tokenized_line:\n",
    "        self.tokens.append(str(token))\n",
    "\n",
    "      text += \" \".join(str(tokenized_line))\n",
    "    file.close()\n",
    "\n",
    "    self.raw_text = raw_text\n",
    "    self.text = text\n",
    "\n",
    "  def __create_word_dict(self):\n",
    "    \"\"\"\n",
    "    Creates a dictionary to store the frequency of each word in the document.\n",
    "    \"\"\"\n",
    "    for token in self.tokens:\n",
    "      self.word_dict[token] = self.word_dict.get(token, 0) + 1\n",
    "  \n",
    "  def __get_one_line_text(self):\n",
    "    str_tokens = []\n",
    "    for token in self.tokens:\n",
    "      str_tokens.append(str(token))\n",
    "\n",
    "    self.one_line_text = \" \".join(str_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def generate_documents(class_name: str, directory: str = \"\") -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Generate a table of document comparison metrics for a given class name.\n",
    "\n",
    "    Parameters:\n",
    "    class_name (str): The name of the class.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing comparison metrics for each document pair.\n",
    "    \"\"\"\n",
    "\n",
    "    directorio_principal = directory\n",
    "\n",
    "    data = []\n",
    "\n",
    "    ruta_clase = os.path.join(directorio_principal, class_name)\n",
    "\n",
    "    if os.path.exists(ruta_clase):\n",
    "        for carpeta_id in os.listdir(ruta_clase):\n",
    "            ruta_carpeta = os.path.join(ruta_clase, carpeta_id)\n",
    "\n",
    "            if os.path.isdir(ruta_carpeta):\n",
    "                for archivo in os.listdir(ruta_carpeta):\n",
    "                    ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "                    Document(ruta_archivo)\n",
    "                    \n",
    "    else:\n",
    "        print(f'La ruta {ruta_clase} no existe')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name = \"data_set_splitted/\"\n",
    "\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}train\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}train\")\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}val\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}val\")\n",
    "generate_documents(\"no_plagiado\", f\"{dir_name}test\")\n",
    "generate_documents(\"plagiado\", f\"{dir_name}test\")\n",
    "generate_documents(\"no_plagiado\", \"queries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832\n"
     ]
    }
   ],
   "source": [
    "print(len(Document.all_documents))\n",
    "\n",
    "data = []\n",
    "\n",
    "for document in Document.all_documents:  \n",
    "  data.append(document.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "vector_size = 100\n",
    "epochs = 100\n",
    "\n",
    "tagged_data = [TaggedDocument(words=code, tags=[str(i)]) for i, code in enumerate(data)]\n",
    "\n",
    "model = Doc2Vec(vector_size=vector_size, epochs=epochs)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "model.train(tagged_data, \n",
    "  total_examples=model.corpus_count,\n",
    "  epochs=model.epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"doc_2_vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.39622623  0.76118845  1.4314494  -0.05048709  0.17532209  0.46299627\n",
      " -0.34288982 -0.4091335   0.38640738 -0.8947137   1.0105364   0.21155538\n",
      " -0.11648066 -0.5988063   0.7520589   0.3919415  -0.8178045  -0.76191\n",
      " -0.4989501   0.3184066  -0.14023519  0.77493364  0.6081415  -0.23907092\n",
      " -1.4507564  -0.30758205  0.6136947  -0.36435512 -0.09029232 -0.45981783\n",
      "  0.17073308 -0.43309942 -0.68137574  0.49596515 -0.25680435 -0.9114806\n",
      "  1.6861128   0.56535053 -0.59712774  0.00327066  0.19090734 -0.5718223\n",
      "  0.5532488   1.5670247   0.6426095   1.053093    0.02012687  0.6191046\n",
      " -0.11427987 -0.3921164  -0.66345453  0.65601456  0.04763686 -0.7044579\n",
      " -0.51289093  0.5714094   0.04158123  0.3122712  -0.07183295 -0.790684\n",
      " -0.2534778  -0.34256777  0.2479571  -0.78059345 -0.27723572  0.5058676\n",
      "  1.2426094  -0.09071164 -0.28360447  0.7178948  -0.9028868   0.6027275\n",
      "  0.61436576 -0.33075935 -1.0134513   0.4228074   0.5216753   0.22657597\n",
      " -0.14012007 -0.93241066  0.4436081   0.0636168  -0.37429228 -0.84467673\n",
      "  0.99884707  0.24570093 -1.5209919   0.4359274   0.23708738  0.61234546\n",
      "  0.2550899  -0.65502626 -0.2749873   0.26970363 -0.3656284  -0.58392924\n",
      " -0.20592134  0.42960703  0.71672624 -0.1133252 ]\n"
     ]
    }
   ],
   "source": [
    "code_vectors = []\n",
    "for code in data:\n",
    "  code_vectors.append(model.infer_vector(code))\n",
    "\n",
    "print(code_vectors[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
