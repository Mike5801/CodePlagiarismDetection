{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irNBzOncNbA9","outputId":"e6ded034-0115-49a7-cd83-c62ac34edb02"},"outputs":[],"source":["!pip install sctokenizer\n","!pip install nltk\n"]},{"cell_type":"markdown","metadata":{"id":"ch4q2ZQhOkik"},"source":["## Tokenización de código\n","Se utilizó la librería sctokenizer para tokenizar los códigos de Java\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"HkaaRthFOVQt"},"outputs":[],"source":["from sctokenizer import JavaTokenizer, TokenType\n","import math\n","def tokenize_code(code):\n","    \"\"\"\n","    Tokenize Java code using JavaTokenizer from sctokenizer.\n","\n","    Parameters:\n","    code (str): The Java code to tokenize.\n","\n","    Returns:\n","    list: A list of token types or token values.\n","    \"\"\"\n","\n","    tokenizer = JavaTokenizer()\n","    tokens = tokenizer.tokenize(code)\n","    token_list = []\n","    for token in tokens:\n","      if token.token_type == TokenType.COMMENT_SYMBOL:\n","        continue\n","      if token.token_type == TokenType.IDENTIFIER:\n","        token_list.append(token.token_type)\n","      else:\n","        token_list.append(token.token_value)\n","\n","    return token_list"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"HHNESzfEPvnl"},"outputs":[],"source":["import re\n","\n","class Document:\n","  \"\"\"\n","  Represents a document object for storing and processing text content.\n","  \"\"\"\n","  all_documents = []\n","\n","  def __init__(self, doc_name: str) -> None:\n","    \"\"\"\n","    Initializes a Document object with the provided document name.\n","\n","    Args:\n","        doc_name (str): The name of the document file.\n","    \"\"\"\n","    self.doc_name = doc_name\n","    self.word_dict: dict[str, int] = {}\n","    self.raw_text = \"\"\n","    self.text = \"\"\n","    self.tokens: list[str] = []\n","    self.raw_comments: str = \"\"\n","    self.token_comments: list[str] = []\n","    self.one_line_text: str = \"\"\n","\n","    self.__read_text()\n","    self.__create_word_dict()\n","    self.__get_comments(self.raw_text)\n","    self.__get_one_line_text()\n","    Document.all_documents.append(self)\n","\n","  def __get_comments(self, code):\n","    \"\"\"\n","    Extracts comments from the provided code string.\n","\n","    Args:\n","        code (str): The code string to extract comments from.\n","    \"\"\"\n","    single_line_comment_pattern = r'//.*'\n","    multi_line_comment_pattern = r'/\\*[\\s\\S]*?\\*/'\n","\n","    single_line_comments = re.findall(single_line_comment_pattern, code)\n","    multi_line_comments = re.findall(multi_line_comment_pattern, code)\n","\n","    stripped_single_line_comments = [comment.lstrip('//').strip() for comment in single_line_comments]\n","    stripped_multi_line_comments = [re.sub(r'(^/\\*|\\*/$)', '', comment).strip() for comment in multi_line_comments]\n","\n","    all_comments = \"\"\n","    for comment in stripped_single_line_comments:\n","      all_comments += f\"{comment} \"\n","\n","    for comment in stripped_multi_line_comments:\n","      all_comments += f\"{comment}\"\n","\n","    self.raw_comments = all_comments\n","    self.token_comments = self.raw_comments.split(\" \")\n","\n","  def __read_text(self) -> None:\n","    \"\"\"\n","    Reads the text content from the document file.\n","    \"\"\"\n","    raw_text = \"\"\n","    text = \"\"\n","    file = open(f\"{self.doc_name}\", \"r\", encoding=\"utf-8\")\n","    while True:\n","      line = file.readline()\n","      if not line:\n","        break\n","\n","      raw_text += line\n","      tokenized_line = tokenize_code(line)\n","      for token in tokenized_line:\n","        self.tokens.append(str(token))\n","\n","      text += \" \".join(str(tokenized_line))\n","    file.close()\n","\n","    self.raw_text = raw_text\n","    self.text = text\n","\n","  def __create_word_dict(self):\n","    \"\"\"\n","    Creates a dictionary to store the frequency of each word in the document.\n","    \"\"\"\n","    for token in self.tokens:\n","      self.word_dict[token] = self.word_dict.get(token, 0) + 1\n","  \n","  def __get_one_line_text(self):\n","    str_tokens = []\n","    for token in self.tokens:\n","      str_tokens.append(str(token))\n","\n","    self.one_line_text = \" \".join(str_tokens)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"aoWze2_yQEX-"},"outputs":[],"source":["import math\n","\n","class Matrix:\n","  \"\"\"\n","  Provides static methods for performing matrix operations.\n","  \"\"\"\n","  @staticmethod\n","  def transpose_matrix(matrix: list[list[float]]) -> list[list[float]]:\n","    \"\"\"\n","    Transposes a given matrix.\n","\n","    Args:\n","        matrix (list[list[float]]): The matrix to transpose.\n","\n","    Returns:\n","        list[list[float]]: The transposed matrix.\n","    \"\"\"\n","    new_matrix: list[list[float]] = []\n","    for col in range(len(matrix[0])):\n","      new_row = []\n","      for row in range(len(matrix)):\n","        new_row.append(matrix[row][col])\n","      new_matrix.append(new_row)\n","\n","    return new_matrix\n","\n","  @staticmethod\n","  def trace_matrix(matrix: list[list[float]]) -> float:\n","    \"\"\"\n","    Calculates the trace of a square matrix.\n","\n","    Args:\n","        matrix (list[list[float]]): The square matrix to calculate the trace for.\n","\n","    Returns:\n","        float: The trace of the matrix (sum of diagonal elements).\n","\n","    Raises:\n","        ValueError: If the input matrix is not square.\n","    \"\"\"\n","    trace: float = 0\n","    row: int = 0\n","    col: int = 0\n","    for _ in range(len(matrix)):\n","      trace += matrix[row][col]\n","      row += 1\n","      col += 1\n","    return trace\n","\n","  @staticmethod\n","  def equalize_matrixes(matrix_a: list[list[float]], matrix_b: list[list[float]]) -> list[list[list[float]]]:\n","    \"\"\"\n","    Equalizes the dimensions of two matrices by padding with zeros if necessary.\n","\n","    Args:\n","        matrix_a (list[list[float]]): The first matrix.\n","        matrix_b (list[list[float]]): The second matrix.\n","\n","    Returns:\n","        list[list[list[float]]]: A list containing the equalized matrices.\n","    \"\"\"\n","    equalized_matrixes: list[list[list[float]]] = []\n","    matrix_a_col_len: int = len(matrix_a[0])\n","    matrix_b_row_len: int = len(matrix_b)\n","\n","    if matrix_a_col_len > matrix_b_row_len:\n","      diff: int = matrix_a_col_len - matrix_b_row_len\n","      row_zeroes: list[float] = [0] * matrix_a_col_len\n","      col_zeroes: list[float] = [0] * diff\n","\n","      for row in matrix_b:\n","        row += col_zeroes\n","\n","      for _ in range(diff):\n","        matrix_b.append(row_zeroes)\n","\n","    else:\n","      diff: int = matrix_b_row_len - matrix_a_col_len\n","      row_zeroes: list[float] = [0] * matrix_b_row_len\n","      col_zeroes: list[float] = [0] * diff\n","\n","      for row in matrix_a:\n","        row = row + col_zeroes\n","\n","      for _ in range(diff):\n","        matrix_a.append(row_zeroes)\n","\n","    equalized_matrixes = [matrix_a, matrix_b]\n","\n","    return equalized_matrixes\n","\n","\n","  @staticmethod\n","  def multiply_matrix(matrix_a: list[list[float]], matrix_b: list[list[float]]) -> list[list[float]]:\n","    \"\"\"\n","    Multiplies two matrices.\n","\n","    Args:\n","        matrix_a (list[list[float]]): The first matrix.\n","        matrix_b (list[list[float]]): The second matrix.\n","\n","    Returns:\n","        list[list[float]]): The resulting product matrix.\n","\n","    Raises:\n","        ValueError: If the inner dimensions of the matrices are not compatible for multiplication.\n","    \"\"\"\n","    matrix_c: list[list[float]] = []\n","    row_a: int = 0\n","\n","    if len(matrix_a[0]) != len(matrix_b):\n","      matrix_a, matrix_b = Matrix.equalize_matrixes(matrix_a, matrix_b)\n","\n","    for row_a in range(len(matrix_a)):\n","      new_row: list[float] = []\n","      for col_b in range(len(matrix_b[0])):\n","        new_val: float = 0\n","        for col_a in range(len(matrix_a[0])):\n","          new_val += matrix_a[row_a][col_a] * matrix_b[col_a][col_b]\n","        new_row.append(new_val)\n","      matrix_c.append(new_row)\n","\n","    return matrix_c\n","\n","  @staticmethod\n","  def normalize_matrix(matrix: list[list[float]]) -> float:\n","    \"\"\"\n","    Calculates the Frobenius norm of a matrix.\n","\n","    Args:\n","        matrix (list[list[float]]): The matrix to calculate the norm for.\n","\n","    Returns:\n","        float: The Frobenius norm of the matrix.\n","    \"\"\"\n","    matrix_t = Matrix.transpose_matrix(matrix)\n","    matrix_c = Matrix.multiply_matrix(matrix_t, matrix)\n","    trace = Matrix.trace_matrix(matrix_c)\n","\n","    return math.sqrt(trace)\n","\n","  @staticmethod\n","  def print_matrix(matrix: list[list[float]]) -> None:\n","    \"\"\"\n","    Prints a matrix in a formatted way.\n","\n","    Args:\n","        matrix (list[list[float]]): The matrix to print.\n","    \"\"\"\n","    print(\"--------------------------------------------\")\n","    for row in range(len(matrix)):\n","      for col in range(len(matrix[0])):\n","        print(f\"{round(matrix[row][col], 4)} |\", end=\"\")\n","      print(\"\\n\")\n","    print(\"--------------------------------------------\")"]},{"cell_type":"markdown","metadata":{},"source":["## Cadena de Markov\n","Representa la probabilidad de transiciones entre los tokens de un código:\n","- Matrix: contiene los métodos necesarios para hacer operaciones entre matrices\n","- Markov_Chain: contiene la matriz de transición de los tokens de un archivo de código y la información necesaria para hacer la similitud de cosenos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"7K_E73KeQQzI"},"outputs":[],"source":["class Markov_Chain:\n","  \"\"\"\n","  Represents a Markov chain for text generation based on a document.\n","  \"\"\"\n","  def __init__(self, doc_name: str, tokenize: bool = False):\n","    \"\"\"\n","    Initializes a MarkovChain object.\n","\n","    Args:\n","        doc_name (str): The name of the document to build the chain from.\n","        tokenize (bool, optional): Whether to tokenize the document\n","            before processing. Defaults to False.\n","    \"\"\"\n","    self.markov_chain: list[list[float]] = []\n","    self.doc_name = doc_name\n","    self.text = \"\"\n","    self.tokens: list[str] = []\n","    self.token_transitions: dict[str, dict[str, int]] = {}\n","\n","    if tokenize:\n","      self.__tokenize_file()\n","    else:\n","      self.__read_file()\n","\n","    self.__generate_token_transitions()\n","    self.__generate_markov_chain()\n","\n","  def __read_file(self):\n","    \"\"\"\n","    Reads the text content from the document file.\n","    \"\"\"\n","    text = \"\"\n","    file = open(f\"{self.doc_name}\", \"r\", encoding=\"utf-8\")\n","    while True:\n","      line = file.readline()\n","\n","      if not line:\n","        break\n","\n","      tokenized_line = tokenize_code(line)\n","      for token in tokenized_line:\n","        self.tokens.append(token)\n","      text += \" \".join(str(tokenized_line))\n","    file.close()\n","\n","    self.text = text\n","\n","  def __generate_token_transitions(self):\n","    \"\"\"\n","    Generates a dictionary representing token transitions in the document.\n","    \"\"\"\n","    for i, token in enumerate(self.tokens):\n","      if token not in self.token_transitions:\n","        self.token_transitions[token] = {}\n","\n","      if i < len(self.tokens) - 1 and self.tokens[i + 1] not in self.token_transitions[token]:\n","        self.token_transitions[token][self.tokens[i + 1]] = 1\n","      elif i < len(self.tokens) - 1 and self.tokens[i + 1] in self.token_transitions[token]:\n","        self.token_transitions[token][self.tokens[i + 1]] += 1\n","\n","  def __generate_markov_chain(self):\n","    \"\"\"\n","    Generates the Markov chain transition matrix from token transitions.\n","    \"\"\"\n","    self.markov_chain = []\n","\n","    for key in self.token_transitions:\n","      row: list[float] = []\n","      total_transitions: int = 0\n","      for freq in self.token_transitions[key].values():\n","        total_transitions += freq\n","      for next_key in self.token_transitions:\n","        if next_key not in self.token_transitions[key]:\n","          row.append(0)\n","        else:\n","          row.append(self.token_transitions[key][next_key] / total_transitions)\n","      self.markov_chain.append(row)"]},{"cell_type":"markdown","metadata":{},"source":["# Similitudes calculadas en Compare\n","\n","## Distribución de probabilidad (TF-IDF)\n","\n","Fórmula de TF-IDF:\n","$$\n","\\text{tf-idf}(t, d) = \\underbrace{tf(t, d)}{\\text{Term Frequency}} \\times \\underbrace{\\log{10}\\left(\\dfrac{N}{df(t)}\\right)}_{\\text{Inverse Document Frequency}}\n","$$\n","\n","## Similitud de Euclidean\n","A partir de los vectores obtenidos por TF-IDF, se calculó la distancia euclideana para ver qué tan cerca están los dos vectores en una línea recta.\n","\n","La fórmula de la distancia euclidian es:\n","\n","$$d(p,q) = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_n-p_n)^2} = \\sqrt{\\sum_{i=1}^n (q_i-p_i)^2}$$\n","\n","## Similitud de Manhattan\n","A partir de los vectores obtenidos por TF-IDF, se calculó la distancia manhattan para ver qué tan cerca están los vectores simulando un camino de una calle.\n","\n","La fórmula de la distancia de Manhattan en LaTeX es:\n","\n","$$d(p,q) = \\sum_{i=1}^n |q_i-p_i|$$\n","\n","## Similitud de Jaccard\n","\n","Calcula la similitud entre dos conjuntos generados a partir de los tokens de un par de códigos\n","\n","$$\n","J(A, B) = \\frac{|A \\cap B|} {|A \\cup B|}\n","$$\n","\n","## Similitud de espacios y saltos de línea\n","Se calcula obteniendo la diferencia de la cuenta de saltos de línea y espacios entre ambos códigos. Posteriormente se suman, y se dividen entre el total de los máximos de cuenta. Esto se expresa de la siguiente manera:\n","\n","$$\\text{Diferencia} = 1 - \\frac{|\\text{NewLines}_A - \\text{NewLines}_B| + |\\text{Spaces}_A - \\text{Spaces}_B|}{\\max(\\text{NewLines}_A, \\text{NewLines}_B) + \\max(\\text{Spaces}_A, \\text{Spaces}_B)}$$\n","\n","## Similitud de llaves\n","La función `classify_braces` clasifica las llaves (`{` y `}`) en un código según su posición en cada línea. Devuelve una cadena de notación de llaves, donde:\n","- '1' significa que la llave está al principio de la línea y hay otros caracteres después\n","- '2' significa que la llave está al final de la línea\n","- '3' significa que la llave está en el medio de la línea\n","- '4' significa que la llave está sola en la línea\n","\n","La función `lcs_length` calcula la longitud de la subsecuencia común más larga (LCS) entre dos cadenas utilizando programación dinámica.\n","\n","La función `calculate_brace_similarity` calcula la similitud de llaves entre dos códigos. Primero, clasifica las llaves en cada código utilizando la función `classify_braces` para obtener las notaciones de llaves. Luego, calcula la longitud de la LCS entre las dos notaciones de llaves utilizando la función `lcs_length`. Finalmente, calcula la similitud de llaves utilizando la siguiente fórmula:\n","\n","$$\\text{BraceSimilarity}(\\text{code1}, \\text{code2}) = \\frac{2 \\times \\text{LCS}(\\text{notation1}, \\text{notation2})}{\\text{L1} \\times \\text{L2}}$$\n","\n","## Similitud del estilo de código\n","Calcula la similitud del estilo de código a partir de la similitud de los parentesis, comentarios, saltos de línea y espacios\n","$$\n","\\text{code style similarity} =  {(bs + cs + snl) / 3}\n","$$"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"f9fZ-pfxQZN_"},"outputs":[],"source":["from gensim.models import Doc2Vec\n","\n","class Compare:\n","  \"\"\"\n","  Compares documents using TF-IDF (Term Frequency-Inverse Document Frequency).\n","  \"\"\"\n","  word_doc_freq: dict[str, tuple[list[int], set[str]]] = {}\n","  def __init__(self):\n","    \"\"\"\n","    Initializes a Compare object. Sets the number of documents to be compared (default 2).\n","    \"\"\"\n","    self.__n_docs = 2\n","\n","  def __get_global_dict(self, doc_1: Document, doc_2: Document) -> None:\n","    \"\"\"\n","    Creates a dictionary to store word frequencies for both documents.\n","\n","    Args:\n","        doc_1 (Document): The first document to compare.\n","        doc_2 (Document): The second document to compare.\n","    \"\"\"\n","    Compare.word_doc_freq = {}\n","    for word in doc_1.word_dict:\n","      Compare.word_doc_freq[word] = ([doc_1.word_dict[word]], set([doc_1.doc_name]))\n","\n","    for word in doc_2.word_dict:\n","      if word not in Compare.word_doc_freq:\n","        Compare.word_doc_freq[word] = ([doc_2.word_dict[word]], set([doc_2.doc_name]))\n","      else:\n","        Compare.word_doc_freq[word][0][0] += doc_2.word_dict[word]\n","        Compare.word_doc_freq[word][1].add(doc_2.doc_name)\n","\n","  def __calc_idf(self) -> list[float]:\n","    \"\"\"\n","    Calculates Inverse Document Frequency (IDF) for each word in the vocabulary.\n","\n","    Returns:\n","        list[float]: A list containing IDF values for all words.\n","    \"\"\"\n","    list_idf = []\n","    for word in Compare.word_doc_freq:\n","      idf = math.log((self.__n_docs) / (len(Compare.word_doc_freq[word][1]) + 1)) + 1\n","      list_idf.append(idf)\n","\n","    return list_idf\n","\n","  def __generate_tf(self, doc: Document) -> list[float]:\n","    \"\"\"\n","    Calculates Term Frequency (TF) for each word in a document.\n","\n","    Args:\n","        doc (Document): The document to calculate TF for.\n","\n","    Returns:\n","        list[float]: A list containing TF values for all words in the document.\n","    \"\"\"\n","    doc_tf: list[float] = []\n","    n_words: int = sum(doc.word_dict.values())\n","\n","    for word in Compare.word_doc_freq:\n","      if word not in doc.word_dict:\n","        doc_tf.append(0)\n","      else:\n","        doc_tf.append(doc.word_dict[word] / n_words)\n","\n","    return doc_tf\n","\n","  def __calc_tf_idf(self, tf: list[float], idf: list[float]) -> list[float]:\n","    \"\"\"\n","    Calculates TF-IDF (Term Frequency-Inverse Document Frequency) for each word.\n","\n","    Args:\n","        tf (list[float]): A list containing TF values.\n","        idf (list[float]): A list containing IDF values.\n","\n","    Returns:\n","        list[float]: A list containing TF-IDF values for all words.\n","    \"\"\"\n","    tf_idf: list[float] = []\n","    for i in range(len(idf)):\n","      tf_idf.append(tf[i] * idf[i])\n","\n","    return tf_idf\n","\n","  @staticmethod\n","  def calc_dot_product(u_vector: list[float], v_vector: list[float]) -> float:\n","    \"\"\"\n","    Calculates the dot product of two vectors.\n","\n","    Args:\n","        u_vector (list[float]): The first vector.\n","        v_vector (list[float]): The second vector.\n","\n","    Returns:\n","        float: The dot product of the two vectors.\n","\n","    Raises:\n","        Exception: If the vectors have different lengths.\n","    \"\"\"\n","    if len(u_vector) != len(v_vector):\n","      raise Exception(\"Length of vectors is not equal\")\n","\n","    product: float = 0\n","    for i in range(len(u_vector)):\n","      product += u_vector[i] * v_vector[i]\n","    return product\n","\n","  @staticmethod\n","  def calc_magnitude(vector: list[float]) -> float:\n","    \"\"\"\n","    Calculates the magnitude (length) of a vector.\n","\n","    Args:\n","        vector (list[float]): The vector to calculate the magnitude for.\n","\n","    Returns:\n","        float: The magnitude of the vector.\n","    \"\"\"\n","    magnitude = 0\n","    for num in vector:\n","      magnitude += num ** 2\n","    return math.sqrt(magnitude)\n","\n","  def get_tf_idf(self, doc_1: Document, doc_2: Document) -> list[list[float]]:\n","    \"\"\"\n","    Generates TF-IDF word vectors for the two documents.\n","\n","    Args:\n","        doc_1 (Document): The first document.\n","        doc_2 (Document): The second document.\n","\n","    Returns:\n","        list[list[float]]: A list containing TF-IDF word vectors for both documents.\n","    \"\"\"\n","    self.__get_global_dict(doc_1, doc_2)\n","    doc_1_tf: list[float] = self.__generate_tf(doc_1)\n","    doc_2_tf: list[float] = self.__generate_tf(doc_2)\n","    idf = self.__calc_idf()\n","    doc_1_tf_idf: list[float] = self.__calc_tf_idf(doc_1_tf, idf)\n","    doc_2_tf_idf: list[float] = self.__calc_tf_idf(doc_2_tf, idf)\n","\n","    len_doc_1 = len(doc_1_tf_idf)\n","    len_doc_2 = len(doc_2_tf_idf)\n","\n","    if len_doc_1 != len_doc_2:\n","      if len_doc_1 > len_doc_2:\n","        diff = len_doc_1 - len_doc_2\n","        for i in range(diff):\n","          doc_2_tf_idf.append(0)\n","      else:\n","        diff = len_doc_2 - len_doc_1\n","        for i in range(diff):\n","          doc_1_tf_idf.append(0)\n","\n","    return [doc_1_tf_idf, doc_2_tf_idf]\n","\n","  def cosine_similarity_vector(self, vector_1: list[float], vector_2: list[float]) -> float:\n","    \"\"\"\n","    Compares the similarity of two documents using cosine similarity.\n","\n","    Args:\n","        vector_1 (list[float]): The first vector\n","        vector_2 (list[float]): The second vector.\n","\n","    Returns:\n","        float: The cosine similarity score between the documents (0.0 to 1.0).\n","    \"\"\"\n","\n","    product: float = Compare.calc_dot_product(vector_1, vector_2)\n","    doc_1_magn: float = Compare.calc_magnitude(vector_1)\n","    doc_2_magn: float = Compare.calc_magnitude(vector_2)\n","\n","    similarity: float = product / (doc_1_magn * doc_2_magn)\n","\n","    return round(similarity, 4)\n","  \n","  def cosine_similarity_matrix(self, matrix_a: list[list[float]], matrix_b: list[list[float]]) -> float:\n","    \"\"\"\n","    Calculates the cosine similarity between two matrices.\n","\n","    Args:\n","        matrix_a (list[list[float]]): The first matrix.\n","        matrix_b (list[list[float]]): The second matrix.\n","\n","    Returns:\n","        float: The cosine similarity between the matrices (0.0 to 1.0).\n","    \"\"\"\n","    norm_matrix_a = Matrix.normalize_matrix(matrix_a)\n","    norm_matrix_b = Matrix.normalize_matrix(matrix_b)\n","    matrix_bt = Matrix.transpose_matrix(matrix_b)\n","    matrix_c = Matrix.multiply_matrix(matrix_bt, matrix_a)\n","    trace = Matrix.trace_matrix(matrix_c)\n","\n","    return round(trace / (norm_matrix_a * norm_matrix_b), 4)\n","  \n","  def euclidean_similarity(self, vector_1: list[float], vector_2: list[float]) -> float:\n","    \"\"\"\n","    Calculate the Euclidean distance between two vectors.\n","\n","    Parameters:\n","    vector_1 (list): The first vector.\n","    vector_2 (list): The second vector.\n","\n","    Returns:\n","    float: The Euclidean distance between the two vectors.\n","\n","    Raises:\n","    ValueError: If the vectors are not of the same length.\n","    \"\"\"\n","    if len(vector_1) != len(vector_2):\n","        raise ValueError(\"Vectors should have the same length\")\n","\n","    sum_squares = sum((a - b) ** 2 for a, b in zip(vector_1, vector_2))\n","    distance = math.sqrt(sum_squares)\n","\n","    return distance\n","  \n","  def manhattan_similarity(self, vector_1: list[float], vector_2: list[float]):\n","    \"\"\"\n","    Calculate the Manhattan distance between two vectors.\n","\n","    Parameters:\n","    vector_1 (list): The first vector.\n","    vector_2 (list): The second vector.\n","\n","    Returns:\n","    float: The Manhattan distance between the two vectors.\n","\n","    Raises:\n","    ValueError: If the vectors are not of the same length.\n","    \"\"\"\n","    if len(vector_1) != len(vector_2):\n","        raise ValueError(\"Vectors should have the same length\")\n","    distance = sum(abs(a - b) for a, b in zip(vector_1, vector_2))\n","\n","    return distance\n","  \n","  def jaccard_similarity(self, tokens_1: list[str], tokens_2: list[str]) -> float:\n","    \"\"\"\n","    Calculates the Jaccard similarity between two sets of tokens.\n","\n","    Args:\n","        tokens_1 (set[str]): The set of tokens from the first document.\n","        tokens_2 (set[str]): The set of tokens from the second document.\n","\n","    Returns:\n","        float: The Jaccard similarity score between the documents (0.0 to 1.0).\n","    \"\"\"\n","    tokens1 = set(tokens_1)\n","    tokens2 = set(tokens_2)\n","\n","    intersection = tokens1.intersection(tokens2)\n","    union = tokens1.union(tokens2)\n","\n","    similarity = len(intersection) / len(union)\n","\n","    return similarity\n","  \n","  def space_new_line_similarity(self, code_1: str, code_2: str) -> float:\n","    \"\"\"\n","    Calculate the similarity between two pieces of code based on spaces, tabs, and newlines.\n","\n","    Parameters:\n","    code_1 (str): The first piece of code.\n","    code_2 (str): The second piece of code.\n","\n","    Returns:\n","    float: The similarity score between the two pieces of code.\n","    \"\"\"\n","    tabs_distance = abs(code_1.count('\\t') - code_2.count('\\t'))\n","    spaces_distance = abs(code_1.count(' ') - code_2.count(' '))\n","    newlines_distance = abs(code_1.count('\\n') - code_2.count('\\n'))\n","\n","    total_tabs = max(code_1.count('\\t'), code_2.count('\\t'))\n","    total_spaces = max(code_1.count(' '), code_2.count(' '))\n","    total_newlines = max(code_1.count('\\n'), code_2.count('\\n'))\n","\n","    ED = tabs_distance + spaces_distance + newlines_distance\n","    total = total_tabs + total_spaces + total_newlines\n","    if total == 0:\n","        SNS = 1.0\n","    else:\n","        SNS = 1 - ED / total\n","    return SNS\n","  \n","  @staticmethod\n","  def classify_braces(code: str) -> str:\n","    \"\"\"\n","    Classify braces in the given code and return a string representation.\n","\n","    Parameters:\n","    code (str): The code to classify braces.\n","\n","    Returns:\n","    str: A string representing the classification of braces in the code.\n","    \"\"\"\n","    brace_notation = []\n","    lines = code.split('\\n')\n","    for line in lines:\n","        stripped = line.strip()\n","        if '{' in stripped or '}' in stripped:\n","            if stripped.startswith('{') or stripped.startswith('}'):\n","                if len(stripped) > 1:\n","                    brace_notation.append('1')\n","                else:\n","                    brace_notation.append('4')\n","            elif stripped.endswith('{') or stripped.endswith('}'):\n","                brace_notation.append('2')\n","            else:\n","                brace_notation.append('3')\n","    return ''.join(brace_notation)\n","\n","  @staticmethod\n","  def lcs(iter_1: list, iter_2: list) -> int:\n","      \"\"\"\n","      Calculate the length of the longest common subsequence (LCS) between two strings.\n","\n","      Parameters:\n","      iter_1 (iterable[any]): The first iterable.\n","      iter_2 (iterable[any]): The second iterable\n","\n","      Returns:\n","      int: The length of the LCS.\n","      \"\"\"\n","      m, n = len(iter_1), len(iter_2)\n","      dp = [[0] * (n + 1) for _ in range(m + 1)]\n","      for i in range(m):\n","          for j in range(n):\n","              if iter_1[i] == iter_2[j]:\n","                  dp[i + 1][j + 1] = dp[i][j] + 1\n","              else:\n","                  dp[i + 1][j + 1] = max(dp[i + 1][j], dp[i][j + 1])\n","      return dp[m][n]\n","  \n","  def calculate_brace_similarity(self, code_1: str, code_2: str) -> float:\n","      \"\"\"\n","      Calculate the similarity between two pieces of code based on their brace notation.\n","\n","      Parameters:\n","      code_1 (str): The first piece of code.\n","      code_2 (str): The second piece of code.\n","\n","      Returns:\n","      float: The similarity score between the two pieces of code.\n","      \"\"\"\n","      notation1 = Compare.classify_braces(code_1)\n","      notation2 = Compare.classify_braces(code_2)\n","\n","      LCS = Compare.lcs(notation1, notation2)\n","      L1, L2 = len(notation1), len(notation2)\n","\n","      if L1 == 0 or L2 == 0:\n","          return 0.0\n","      BS = 2 * LCS / (L1 * L2)\n","      return BS\n","  \n","  def code_style_similarity(self, bs: float, cs: float, snl: float) -> float:\n","    \"\"\"\n","    Calculates a code style similarity score based on three metrics.\n","\n","    Args:\n","        bs (float): The bracket similarity of the code.\n","        cs (float): The comment similarity of the code.\n","        snl (float): The space-newline similarity of the code.\n","\n","    Returns:\n","        float: The combined code style similarity score (average of the three inputs).\n","    \"\"\"\n","    return (bs + cs + snl) / 3\n","  \n","  def levenshtein_distance(self, iter_1: list, iter_2: list) -> float:\n","    m = len(iter_1)\n","    n = len(iter_2)\n","\n","    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n","\n","    for i in range(m + 1):\n","      dp[i][0] = i\n","    for j in range(n + 1):\n","      dp[0][j] = j\n","\n","    for i in range(1, m + 1):\n","       for j in range(1, n + 1):\n","          if iter_1[i - 1] == iter_2[j - 1]:\n","            dp[i][j] = dp[i - 1][j - 1]\n","          else:\n","             dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n","    \n","    return dp[m][n]\n","  \n","  def get_doc_2_vec(self, token_vector_1: list[str], token_vector_2: list[str]) -> list[list[str]]:\n","    model = Doc2Vec.load(\"doc_2_vec_model\")\n","    vector_1 = model.infer_vector(token_vector_1)\n","    vector_2 = model.infer_vector(token_vector_2)\n","\n","    return [vector_1, vector_2]\n","     \n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"hCgnq_z2ptq6"},"outputs":[],"source":["import os\n","import shutil\n","\n","def table_generator(class_name: str, directory: str = \"\") -> list[dict[str, float]]:\n","    \"\"\"\n","    Generate a table of document comparison metrics for a given class name.\n","\n","    Parameters:\n","    class_name (str): The name of the class.\n","\n","    Returns:\n","    list: A list of dictionaries containing comparison metrics for each document pair.\n","    \"\"\"\n","\n","    directorio_principal = directory\n","\n","    data = []\n","\n","    compare = Compare()\n","\n","    ruta_clase = os.path.join(directorio_principal, class_name)\n","\n","    if os.path.exists(ruta_clase):\n","        for carpeta_id in os.listdir(ruta_clase):\n","            ruta_carpeta = os.path.join(ruta_clase, carpeta_id)\n","\n","            if os.path.isdir(ruta_carpeta):\n","                archivos = []\n","                for archivo in os.listdir(ruta_carpeta):\n","                    ruta_archivo = os.path.join(ruta_carpeta, archivo)\n","                    archivos.append(ruta_archivo)\n","\n","                doc_1 = Document(archivos[0])\n","                doc_2 = Document(archivos[1])\n","                doc_1_mkv = Markov_Chain(archivos[0])\n","                doc_2_mkv = Markov_Chain(archivos[1])\n","\n","                # vector_1, vector_2 = compare.get_tf_idf(doc_1, doc_2)\n","                vector_1, vector_2 = compare.get_doc_2_vec(doc_1.tokens, doc_2.tokens)\n","                cos_vector = compare.cosine_similarity_vector(vector_1, vector_2)\n","                cos_markov = compare.cosine_similarity_matrix(doc_1_mkv.markov_chain, doc_2_mkv.markov_chain)\n","                euc = compare.euclidean_similarity(vector_1, vector_2)\n","                mht = compare.manhattan_similarity(vector_1, vector_2)\n","                jac = compare.jaccard_similarity(doc_1.tokens, doc_2.tokens)\n","                snl = compare.space_new_line_similarity(doc_1.raw_text, doc_2.raw_text)\n","                bs = compare.calculate_brace_similarity(doc_1.raw_text, doc_2.raw_text)\n","                cs = compare.jaccard_similarity(doc_1.token_comments, doc_2.token_comments)\n","                style_similarity = compare.code_style_similarity(bs, cs, snl)\n","                lcs_score = compare.lcs(doc_1.tokens, doc_2.tokens) / len(doc_2.tokens)\n","                levenshtein_score = compare.levenshtein_distance(doc_1.tokens, doc_2.tokens) / len(doc_2.tokens)\n","\n","                veredict = 1 if class_name == \"plagiado\" else 0\n","\n","                _map = {\n","                    \"Doc2Vec\": cos_vector,\n","                    \"Markov\": cos_markov,\n","                    \"Euclidean\" : euc,\n","                    \"Manhattan\": mht,\n","                    \"Jaccard\": jac,\n","                    \"Space_NewLine\": snl,\n","                    \"BraceSimilarity\": bs,\n","                    \"CommentSimilarity\": cs,\n","                    \"CodeStyleSimilarity\": style_similarity,\n","                    \"LCS_Score\": lcs_score,\n","                    \"LevenshteinScore\": levenshtein_score,\n","                    \"Veredict\": veredict }\n","\n","                data.append(_map)\n","    else:\n","        print(f'La ruta {ruta_clase} no existe')\n","\n","    return data"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"oEJgfxH901Qi"},"outputs":[],"source":["import csv\n","def generate_table(data: list[dict[str, float]], output_name: str) -> None:\n","  \"\"\"\n","    Generate a CSV table from the provided data.\n","\n","    Parameters:\n","    data (list of dict): The data to be written into the CSV file.\n","\n","    Returns:\n","    None\n","  \"\"\"\n","  csv_file = f'{output_name}.csv'\n","\n","  campos = [\n","    \"Doc2Vec\",\n","    \"Markov\",\n","    \"Euclidean\",\n","    \"Manhattan\",\n","    \"Jaccard\",\n","    \"Space_NewLine\",\n","    \"BraceSimilarity\",\n","    \"CommentSimilarity\",\n","    \"CodeStyleSimilarity\",\n","    \"LCS_Score\",\n","    \"LevenshteinScore\",\n","    \"Veredict\"]\n","\n","  with open(csv_file, mode='w', newline='') as file:\n","      writer = csv.DictWriter(file, fieldnames=campos)\n","\n","      writer.writeheader()\n","\n","      for row in data:\n","          writer.writerow(row)\n","\n","  print(f'Archivo {csv_file} creado exitosamente.')"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[-0.23487532  1.2888407   0.9773964   2.0828087  -0.5498208  -1.1947503\n","  0.60770047 -0.90992045  0.05750665 -0.6877957  -0.01889475  2.1811159\n","  1.0289016   0.11861807 -0.49091274 -0.059469    0.3286159   0.03399688\n"," -0.81135345  0.62422943 -0.12692899  2.0113661   0.4618638  -0.6277114\n","  0.0567116   0.48162276 -0.5948959  -0.2650232  -0.43622696 -0.23889601\n"," -1.2884675   0.20290206  1.0441239   0.68333125  1.7129894  -0.8224682\n"," -1.0937785  -0.85413337  0.81623137 -0.50794476 -0.12729672  1.3159592\n","  0.27488938  0.23508945 -0.43936047  1.016698   -1.0261252  -0.72687405\n","  0.5828255  -0.33677155 -0.09925272 -0.0795472   0.65745914  0.6303941\n"," -0.528853    0.05797065  0.3941228   2.0790372   1.3805856   0.33475447]\n"]}],"source":["#------------------PLAYGROUND---------------\n","\n","compare = Compare()\n","doc_1 = Document(\"queries/plagiado/2/og_2.java\")\n","doc_2 = Document(\"queries/plagiado/2/cp_2.java\")\n","\n","new_metric = compare.levenshtein_distance(doc_1.tokens, doc_2.tokens)\n","doc_2_len = len(doc_2.tokens)\n","\n","v1, v2 = compare.get_doc_2_vec(doc_1.tokens, doc_2.tokens)\n","\n","print(v1)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"jEdtZKE1urWT"},"outputs":[],"source":["splits = [\"train\", \"val\", \"test\", \"queries\"]\n","curr_split = splits[3]\n","dir_name = f\"data_set_splitted/{curr_split}\" if curr_split != \"queries\" else curr_split\n","\n","data1 = table_generator(\"plagiado\", dir_name)\n","data2 = table_generator(\"no_plagiado\", dir_name)\n","data = data1 + data2"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"psIEgp_v0y2R","outputId":"b99f933b-0e2d-49f7-b296-f21f385bc38c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'Doc2Vec': 0.9842, 'Markov': 1.0, 'Euclidean': 0.7779753923815512, 'Manhattan': 4.810112617909908, 'Jaccard': 0.9047619047619048, 'Space_NewLine': 0.9358974358974359, 'BraceSimilarity': 0.3333333333333333, 'CommentSimilarity': 0.21428571428571427, 'CodeStyleSimilarity': 0.4945054945054945, 'LCS_Score': 0.9838709677419355, 'LevenshteinScore': 0.016129032258064516, 'Veredict': 1}, {'Doc2Vec': 0.9861, 'Markov': 1.0, 'Euclidean': 1.1264646905563314, 'Manhattan': 6.6635792795568705, 'Jaccard': 0.9393939393939394, 'Space_NewLine': 0.9952380952380953, 'BraceSimilarity': 0.13333333333333333, 'CommentSimilarity': 0.17647058823529413, 'CodeStyleSimilarity': 0.4350140056022409, 'LCS_Score': 0.9941176470588236, 'LevenshteinScore': 0.0058823529411764705, 'Veredict': 1}, {'Doc2Vec': 0.6083, 'Markov': 0.2987, 'Euclidean': 6.8364974658692885, 'Manhattan': 40.08920247852802, 'Jaccard': 0.72, 'Space_NewLine': 0.3950762016412661, 'BraceSimilarity': 0.045, 'CommentSimilarity': 0.0, 'CodeStyleSimilarity': 0.14669206721375536, 'LCS_Score': 0.6270627062706271, 'LevenshteinScore': 0.48514851485148514, 'Veredict': 1}, {'Doc2Vec': 0.9937, 'Markov': 1.0, 'Euclidean': 1.2033949592023263, 'Manhattan': 7.2310379557311535, 'Jaccard': 0.7391304347826086, 'Space_NewLine': 0.9980119284294234, 'BraceSimilarity': 0.06451612903225806, 'CommentSimilarity': 1.0, 'CodeStyleSimilarity': 0.6875093524872272, 'LCS_Score': 0.9829059829059829, 'LevenshteinScore': 0.017094017094017096, 'Veredict': 1}, {'Doc2Vec': 0.9856, 'Markov': 0.9946, 'Euclidean': 1.631884571887044, 'Manhattan': 10.482286550104618, 'Jaccard': 1.0, 'Space_NewLine': 0.693661971830986, 'BraceSimilarity': 0.0763888888888889, 'CommentSimilarity': 0.14285714285714285, 'CodeStyleSimilarity': 0.3043026678590059, 'LCS_Score': 1.0, 'LevenshteinScore': 0.002145922746781116, 'Veredict': 1}, {'Doc2Vec': 0.4826, 'Markov': 0.4592, 'Euclidean': 6.0670641686742535, 'Manhattan': 36.762526504695415, 'Jaccard': 0.5666666666666667, 'Space_NewLine': 0.703448275862069, 'BraceSimilarity': 0.3333333333333333, 'CommentSimilarity': 0.125, 'CodeStyleSimilarity': 0.3872605363984674, 'LCS_Score': 0.7777777777777778, 'LevenshteinScore': 0.4567901234567901, 'Veredict': 0}, {'Doc2Vec': 0.5776, 'Markov': 0.8144, 'Euclidean': 5.835297113658628, 'Manhattan': 35.659312307834625, 'Jaccard': 0.5862068965517241, 'Space_NewLine': 0.7016129032258065, 'BraceSimilarity': 0.5, 'CommentSimilarity': 0.06666666666666667, 'CodeStyleSimilarity': 0.4227598566308244, 'LCS_Score': 0.8679245283018868, 'LevenshteinScore': 0.8490566037735849, 'Veredict': 0}, {'Doc2Vec': 0.5175, 'Markov': 0.4639, 'Euclidean': 7.426471117113009, 'Manhattan': 41.71271547675133, 'Jaccard': 0.9069767441860465, 'Space_NewLine': 0.22085201793721976, 'BraceSimilarity': 0.09070294784580499, 'CommentSimilarity': 0.16666666666666666, 'CodeStyleSimilarity': 0.1594072108165638, 'LCS_Score': 0.7785714285714286, 'LevenshteinScore': 0.26666666666666666, 'Veredict': 0}, {'Doc2Vec': 0.4411, 'Markov': 0.3375, 'Euclidean': 10.53233916692724, 'Manhattan': 67.9172875136137, 'Jaccard': 0.5, 'Space_NewLine': 0.2919621749408984, 'BraceSimilarity': 0.08333333333333333, 'CommentSimilarity': 0.02040816326530612, 'CodeStyleSimilarity': 0.13190122384651262, 'LCS_Score': 0.40987124463519314, 'LevenshteinScore': 0.6416309012875536, 'Veredict': 0}, {'Doc2Vec': 0.9277, 'Markov': 0.3388, 'Euclidean': 4.042766701529118, 'Manhattan': 25.234111208468676, 'Jaccard': 0.66, 'Space_NewLine': 0.8667763157894737, 'BraceSimilarity': 0.04878048780487805, 'CommentSimilarity': 1.0, 'CodeStyleSimilarity': 0.6385189345314506, 'LCS_Score': 0.7761904761904762, 'LevenshteinScore': 0.2761904761904762, 'Veredict': 0}]\n","Archivo queries.csv creado exitosamente.\n"]}],"source":["print(data)\n","generate_table(data, curr_split)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
